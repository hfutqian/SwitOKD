
## The implementation of compared methods
To validate the effectiveness of SwitOKD, we implement the compared methods, including online (i.e., DML[1], KDCL[2]) and offline fashions (i.e., KD[3], FitNet[[4], AT[5], CRD[6], RCO[7]), by using the same student-teacher network pair with a large distillation gap for fair comparison. The codes of the compared methods can be acccessible from the following references:

[1] Zhang, Y., Xiang, T., Hospedales, T.M., Lu, H.: Deep mutual learning. In: Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.pp. 4320–4328 (2018)

[2] Guo, Q., Wang, X., Wu, Y., Yu, Z., Liang, D., Hu, X., Luo, P.: Online knowledge distillation via collaborative learning. In: IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) (June 2020)

[3] Hinton, G., Vinyals, O., Dean, J.: Distilling the knowledge in a neural network. In NIPS (2015)

[4] Romero, A., Ballas, N., Kahou, S.E., Chassang, A., Gatta, C., Bengio, Y.: Fitnets: Hints for thin deep nets. arXiv preprint arXiv:1412.6550 (2014)

[5] Zagoruyko, S., Komodakis, N.: Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. arXiv preprint
arXiv:1612.03928 (2016)

[6] Tian, Y., Krishnan, D., Isola, P.: Contrastive representation distillation. arXiv preprint arXiv:1910.10699 (2019)

[7] Jin, X., Peng, B., Wu, Y., Liu, Y., Liu, J., Liang, D., Yan, J., Hu, X.: Knowledge distillation via route constrained optimization. In: Proceedings of the IEEE
International Conference on Computer Vision. pp. 1345–1354 (2019)














